{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sE1VOdomM5JH"
      },
      "outputs": [],
      "source": [
        "#Contractions in NLP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install contractions\n",
        "import contractions\n",
        "# contracted text\n",
        "text = '''I'll be there within 5 min. Shouldn't you be there too?\n",
        "          I'd love to see u there my dear. It's awesome to meet new friends.\n",
        "          We've been waiting for this day for so long.'''\n",
        "\n",
        "# creating an empty list\n",
        "expanded_words = []\n",
        "for word in text.split():\n",
        "  # using contractions.fix to expand the shortened words\n",
        "  expanded_words.append(contractions.fix(word))\n",
        "\n",
        "expanded_text = ' '.join(expanded_words)\n",
        "print('Original text: ' + text)\n",
        "print('Expanded_text: ' + expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQnwoEWtM_pW",
        "outputId": "8d678499-391c-417f-8dec-68ec7b42e6cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: I'll be there within 5 min. Shouldn't you be there too? \n",
            "          I'd love to see u there my dear. It's awesome to meet new friends.\n",
            "          We've been waiting for this day for so long.\n",
            "Expanded_text: I will be there within 5 min. Should not you be there too? I would love to see you there my dear. It is awesome to meet new friends. We have been waiting for this day for so long.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ambiguity of contractions**\n",
        "---\n",
        "\n",
        "It's observed that some contractions represent multiple word combinations.\n",
        "\n",
        "*\"ain't\": \"am not / are not / is not / has not / have not\"*\n",
        "\n",
        "**Solution to the above:**\n",
        "\n",
        "The pycontractions library\n",
        "We can also use the pycontractions library to expand the contractions. It works in the following way:\n",
        "\n",
        "Case 1: If a contraction corresponds to only one sequence of words, pycontractions replaces the contraction with that word sequence.\n",
        "\n",
        "Case 2: If a contraction corresponds to many possible expansions. Then, in that case, pycontractions produces all the possible expansions and then uses a spell checker. The grammatically incorrect options are discarded, and the correct choice is selected.\n",
        "\n",
        "It has been observed that pycontractions is more accurate than the contractions library of python as it takes into account the grammar of the text.\n",
        "\n",
        "https://pypi.org/project/pycontractions/\n"
      ],
      "metadata": {
        "id": "fJTZU20NNGmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "\n",
        "text = '''I ain't doing that.'''\n",
        "\n",
        "expanded_text = []\n",
        "for word in text.split():\n",
        "  expanded_text.append(contractions.fix(word))\n",
        "\n",
        "expanded_text = ' '.join(expanded_text)\n",
        "print('Input : ' + text)\n",
        "print('\\n')\n",
        "print('Output: ' + expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt49d7q3NBff",
        "outputId": "bfa5338c-04ae-432c-bf4f-7a4bc5a05268"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : I ain't doing that.\n",
            "\n",
            "\n",
            "Output: I are not doing that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tokenization*"
      ],
      "metadata": {
        "id": "cH1HpjkKNRyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'LLMs aka Large Language Models have been the talk of the town for some time.'\n",
        "words = text.split(\" \") # split the text on spaces\n",
        "\n",
        "tokens = {v: k for k, v in enumerate(words)} # generate a word to index mapping"
      ],
      "metadata": {
        "id": "C06BXGuhNJjA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mapped every word in the text to a numerical index\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvN59x1RNP0L",
        "outputId": "fe31aad6-db9a-4dd8-c987-a699d34a78be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'LLMs': 0,\n",
              " 'aka': 1,\n",
              " 'Large': 2,\n",
              " 'Language': 3,\n",
              " 'Models': 4,\n",
              " 'have': 5,\n",
              " 'been': 6,\n",
              " 'the': 10,\n",
              " 'talk': 8,\n",
              " 'of': 9,\n",
              " 'town': 11,\n",
              " 'for': 12,\n",
              " 'some': 13,\n",
              " 'time.': 14}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte Pair Encoding"
      ],
      "metadata": {
        "id": "nGhaYXoRPPx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    return list(text)\n",
        "\n",
        "def get_pair_frequencies(tokens):\n",
        "    pairs = Counter()\n",
        "    for i in range(len(tokens) - 1):\n",
        "        pair = (tokens[i], tokens[i+1])\n",
        "        pairs[pair] += 1\n",
        "    return pairs\n",
        "\n",
        "def merge_most_frequent_pair(tokens, max_merge_operations):\n",
        "    final_tokens = list(tokens)  # Copy the initial tokens into final_tokens\n",
        "    new_tokens = []  # Initialize a list to store the new tokens\n",
        "    for _ in range(max_merge_operations):\n",
        "        pair_frequencies = get_pair_frequencies(final_tokens)  # Count pairs in the current final_tokens\n",
        "        if not pair_frequencies:\n",
        "            break\n",
        "        most_frequent_pair = max(pair_frequencies, key=pair_frequencies.get)\n",
        "        new_token = ''.join(most_frequent_pair)\n",
        "        merged_tokens = []\n",
        "        i = 0\n",
        "        while i < len(final_tokens):\n",
        "            if i < len(final_tokens) - 1 and (final_tokens[i], final_tokens[i+1]) == most_frequent_pair:\n",
        "                merged_tokens.append(new_token)\n",
        "                i += 2\n",
        "            else:\n",
        "                merged_tokens.append(final_tokens[i])\n",
        "                i += 1\n",
        "        final_tokens = merged_tokens\n",
        "        new_tokens.append(new_token)  # Add the new token to the list\n",
        "    return final_tokens, new_tokens  # Return both final_tokens and new_tokens\n",
        "\n",
        "text = \"car,cable,table,watch,chair,mouse\"\n",
        "tokens = tokenize(text)\n",
        "final_tokens, new_tokens = merge_most_frequent_pair(tokens, 15)\n",
        "print(\"Final Tokens:\", final_tokens)\n",
        "print(\"New Tokens:\", new_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO-O2N7LPEyF",
        "outputId": "7f96f6ad-74c3-4226-87e7-99bb3fd8047d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Tokens: ['car,cable,table,wat', 'ch', ',', 'ch', 'a', 'i', 'r,', 'm', 'o', 'u', 's', 'e']\n",
            "New Tokens: ['ca', 'r,', 'bl', 'ble', 'ble,', 'ch', 'car,', 'car,ca', 'car,cable,', 'car,cable,t', 'car,cable,ta', 'car,cable,table,', 'car,cable,table,w', 'car,cable,table,wa', 'car,cable,table,wat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqDNisDyPXaj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}